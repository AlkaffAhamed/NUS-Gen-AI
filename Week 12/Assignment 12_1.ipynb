{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8fbe0df-6d8c-4d19-b15a-2ff9a1cb1dc4",
   "metadata": {},
   "source": [
    "# Week 12: Capstone Project Part 5.1\n",
    "\n",
    "- Done by: A Alkaff Ahamed\n",
    "- Grade: Pending\n",
    "- 1 July 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a937b9f-2b6c-43ee-bb1a-e8dd960692ce",
   "metadata": {},
   "source": [
    "## Learning Outcome Addressed\n",
    "- Apply techniques for effective communication and coordination among multiple agents\n",
    "- Leverage LLMs for task planning, execution and autonomous problem-solving\n",
    "- Assess the limitations, challenges and emerging trends in multi-agent AI systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0c9c9-3eec-4460-b5d7-fdfbdf8be7e0",
   "metadata": {},
   "source": [
    "Exercise: Weather Information Agent \n",
    "\n",
    "## Objective \n",
    "\n",
    "Build a basic function-calling agent that retrieves and processes weather information, demonstrating the practical implementation of agent capabilities. \n",
    "\n",
    "**Prerequisites** \n",
    "\n",
    "- OpenAI API key \n",
    "- WeatherAPI.com free account \n",
    "- Python 3.8+ \n",
    "- Requests library\n",
    "\n",
    "**Setup** \n",
    "\n",
    "1. Create `config.py`: \n",
    "\n",
    "```\n",
    "OPENAI_API_KEY = \"your-key-here\" \n",
    "WEATHER_API_KEY = \"your-weather-api-key\" \n",
    "```\n",
    "\n",
    "2. Install requirements: \n",
    "\n",
    "```\n",
    "pip install openai==0.28 requests python-dotenv\n",
    "```\n",
    "\n",
    "## Exercise Structure\n",
    "\n",
    "Part 1: Setting Up Weather API Integration \n",
    "\n",
    "First, let‚Äôs create the basic weather API integration. Create `weather_agent.py` and add: \n",
    "\n",
    "```python\n",
    "import requests \n",
    "from config import WEATHER_API_KEY \n",
    "\n",
    "def get_weather(location): \n",
    "  try: \n",
    "    url = f\"http://api.weatherapi.com/v1/current.json\" \n",
    "    params = { \n",
    "      \"key\": WEATHER_API_KEY, \n",
    "      \"q\": location, \n",
    "      \"aqi\": \"no\" \n",
    "    } \n",
    "    response = requests.get(url, params=params) \n",
    "    response.raise_for_status() # Raise exception for bad status codes \n",
    "    return response.json() \n",
    "  except requests.exceptions.RequestException as e: \n",
    "    raise Exception(f\"Weather API error: {str(e)}\") \n",
    "```\n",
    "\n",
    "Test Part 1: Add this code at the bottom of the file and run it: \n",
    "\n",
    "```python\n",
    "# Test weather API integration\n",
    "if __name__ == \"__main__\": \n",
    "  try \n",
    "    weather_data = get_weather(\"London\") \n",
    "    print(f\"Weather in {weather_data['location']['name']}:\") \n",
    "    print(f\"Temperature: {weather_data['current']['temp_c']}¬∞C\") \n",
    "    print(f\"Condition: {weather_data['current']['condition']['text']}\") \n",
    "  except Exception as e: \n",
    "    print(f\"Error: {e}\") \n",
    "```\n",
    "\n",
    "Part 2: Adding OpenAI Function Definition \n",
    "\n",
    "Now add the OpenAI integration and function definition: \n",
    "\n",
    "```python\n",
    "import openai \n",
    "from config import OPENAI_API_KEY \n",
    "\n",
    "openai.api_key = OPENAI_API_KEY \n",
    "\n",
    "# Define the function that OpenAI can call\n",
    "functions = [ \n",
    "  { \n",
    "    \"name\": \"get_weather\", \n",
    "    \"description\": \"Get current weather for a location\", \n",
    "    \"parameters\": { \n",
    "      \"type\": \"object\", \n",
    "      \"properties\": { \n",
    "        \"location\": { \n",
    "          \"type\": \"string\", \n",
    "          \"description\": \"City name or location\" \n",
    "        } \n",
    "      }, \n",
    "      \"required\": [\"location\"] \n",
    "    } \n",
    "  } \n",
    "] \n",
    "```\n",
    "\n",
    "Test Part 2: Add this code at the bottom and run it: \n",
    "\n",
    "```python\n",
    "# Test OpenAI function definition\n",
    "if __name__ == \"__main__\": \n",
    "  try: \n",
    "    response = openai.ChatCompletion.create( \n",
    "      model=\"gpt-4\", \n",
    "      messages=[{\"role\": \"user\", \"content\": \"What's the weather like in Tokyo?\"}], \n",
    "      functions=functions, \n",
    "      function_call=\"auto\" \n",
    "    ) \n",
    "    print(\"OpenAI Response:\") \n",
    "    print(response.choices[0].message.function_call) \n",
    "  except Exception as e: \n",
    "    print(f\"Error: {e}\") \n",
    "```\n",
    "\n",
    "Part 3: Implementing the Weather Agent (7 minutes) \n",
    "\n",
    "Now let‚Äôs combine everything into a complete weather agent: \n",
    "\n",
    "```python\n",
    "def weather_agent(user_query): \n",
    "  try \n",
    "    # Get function call from OpenAI\n",
    "    response = openai.ChatCompletion.create( \n",
    "      model=\"gpt-4\", \n",
    "      messages=[{\"role\": \"user\", \"content\": user_query}], \n",
    "      functions=functions, \n",
    "      function_call=\"auto\" \n",
    "    ) \n",
    "     \n",
    "    # Extract function call\n",
    "    function_call = response.choices[0].message.function_call \n",
    "     \n",
    "    # Execute function\n",
    "    if function_call.name == \"get_weather\": \n",
    "      # Parse the arguments from JSON string\n",
    "      import json \n",
    "      arguments = json.loads(function_call.arguments) \n",
    "      weather_data = get_weather(arguments[\"location\"]) \n",
    "       \n",
    "      # Format response\n",
    "      return f\"Current weather in {weather_data['location']['name']}: \" \\ \n",
    "          f\"{weather_data['current']['temp_c']}¬∞C, \" \\ \n",
    "          f\"{weather_data['current']['condition']['text']}\" \n",
    "  except Exception as e: \n",
    "    return f\"Error: {str(e)}\" \n",
    "```\n",
    "\n",
    "Final Test: Replace the previous test code with: \n",
    "\n",
    "```python\n",
    "# Test the complete weather agent\n",
    "if __name__ == \"__main__\": \n",
    "  test_queries = [ \n",
    "    \"What's the weather like in Singapore?\", \n",
    "    \"Is it raining in London right now?\", \n",
    "    \"Tell me the temperature in New York\" \n",
    "  ] \n",
    "\n",
    "  for query in test_queries: \n",
    "    print(f\"\\nQuery: {query}\") \n",
    "    print(f\"Response: {weather_agent(query)}\") \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01084875-4b22-49be-8e0a-856d5d9eb3b9",
   "metadata": {},
   "source": [
    "**Troubleshooting Guide** \n",
    "\n",
    "If you encounter issues at each step, check: \n",
    "\n",
    "Part 1: - Is your WEATHER_API_KEY correctly set in config.py? - Can you access weatherapi.com in your browser? - Are you getting a valid JSON response? \n",
    "\n",
    "Part 2: - Is your OPENAI_API_KEY correctly set? - Are you using the correct OpenAI model? - Check the function definition format \n",
    "\n",
    "Part 3: - Are all imports present at the top of the file? - Is the JSON parsing working correctly? - Are you getting the expected weather data format? \n",
    "\n",
    "**Extension Activities (if time permits)** \n",
    "\n",
    "Add more weather data to the response: \n",
    "\n",
    "- Wind speed and direction \n",
    "- Humidity \n",
    "- Feels like temperature \n",
    "- Add error handling for specific cases: \n",
    "- Invalid city names \n",
    "- API rate limiting \n",
    "- Network timeouts \n",
    "- Improve the response format: \n",
    "- Add weather emojis (üåû, üåßÔ∏è, etc.) \n",
    "- Format temperatures (both ¬∞C and ¬∞F) \n",
    "- Add time of weather reading \n",
    "\n",
    "**Testing Requirements** \n",
    "\n",
    "Basic Queries \n",
    "\n",
    "- Test with simple location queries \n",
    "- Verify temperature and condition output \n",
    "- Check city name handling \n",
    "- Error Cases \n",
    "- Invalid locations \n",
    "- API failures \n",
    "- Malformed queries \n",
    "- Response Format \n",
    "- Verify consistent output format \n",
    "- Check temperature units \n",
    "- Validate weather condition text \n",
    "\n",
    "**Common Issues and Solutions** \n",
    "\n",
    "API Key Issues \n",
    "\n",
    "- Verify keys are correctly set \n",
    "- Check API rate limits \n",
    "- Ensure proper error handling \n",
    "- Location Handling \n",
    "- Handle ambiguous locations \n",
    "- Support different location formats \n",
    "- Manage invalid locations \n",
    "- Response Processing \n",
    "- Handle missing data \n",
    "- Manage API timeouts \n",
    "- Format unexpected responses \n",
    "\n",
    "**Submission Requirements** \n",
    "\n",
    "Create a document containing: \n",
    "\n",
    "- Your working code \n",
    "- Screenshot of successful query responses \n",
    "- Brief explanation of error handling \n",
    "- Any improvements you implemented \n",
    "\n",
    "Remember: Keep your API keys secure and never commit them to version control! \n",
    "\n",
    "**Please note** \n",
    "\n",
    "Note: The field of AI is evolving rapidly. By the time you work on these exercises: \n",
    "\n",
    "- APIs and services mentioned may have changed or been discontinued \n",
    "- Free tiers might no longer be available \n",
    "- Pricing structures could be different \n",
    "- Python libraries may have new versions with different syntax \n",
    "- Example code might need adaptation \n",
    "\n",
    "The core concepts remain valid, but you may need to: \n",
    "\n",
    "- Find alternative services \n",
    "- Adapt code to current API versions \n",
    "- Use different model versions \n",
    "- Modify prompts for newer models \n",
    "- Research current best practices \n",
    "\n",
    "This is normal in AI development. Learning to adapt to these changes is part of working with AI tools. \n",
    "\n",
    "**Resources:** \n",
    "\n",
    "- Check current documentation \n",
    "- Review service status pages \n",
    "- Join developer communities \n",
    "- Research alternatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd068489-10a5-41f9-8cca-b6d8a42ac665",
   "metadata": {},
   "source": [
    "**Estimated time:** 60-90 minutes\n",
    "\n",
    "**Submission Instructions:**\n",
    "\n",
    "- Select the **Start Assignment** button at the top right of this page.\n",
    "- Upload your answers in the form of a Word or PDF file.\n",
    "- Select the **Submit Assignment** button to submit your responses.\n",
    "\n",
    "*This is a graded and counts towards programme completion. You may attempt this assignment only once.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ae036-778b-4703-81f5-ed1d52cb57a5",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaebe395-0346-426c-9170-cfa45c8fc9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alkaff\\anaconda3\\envs\\lc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor, create_react_agent, initialize_agent, AgentType\n",
    "from langchain.tools import tool # for @tool\n",
    "# from langchain_core.tools import Tool # Traditional Tool class\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "\n",
    "# Transformers for Qwen\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "# OpenAI Models: from langchain_openai import ChatOpenAI\n",
    "# Gemini Models: from langchain_google_palm import ChatGooglePalm\n",
    "# Ollama Models: from langchain_ollama import ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d336d982-0ae6-4f29-bd1e-528a75953256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fffbe7-0be3-45a6-bb40-a625a4942d13",
   "metadata": {},
   "source": [
    "## üìå Part 1: Setting Up Weather API Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814d2b90-3b6c-43e6-b4fb-6ae44e24126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_API = os.getenv(\"WEATHER_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1816c394-282b-4805-8eb7-c8ee2f1a061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather Tool\n",
    "# ------------\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get current weather for a location.\"\"\"\n",
    "    try:\n",
    "        url = \"http://api.weatherapi.com/v1/current.json\"\n",
    "        params = {\n",
    "            \"key\": WEATHER_API,\n",
    "            \"q\": location,\n",
    "            \"aqi\": \"no\"\n",
    "        }\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Format the response string to pass to Qwen\n",
    "        result = (\n",
    "            f\"Weather in {data['location']['name']}: \"\n",
    "            f\"{data['current']['temp_c']}¬∞C, \"\n",
    "            f\"{data['current']['condition']['text']}\"\n",
    "        )\n",
    "        #result = data\n",
    "        return result\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Weather API error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a34b76-c303-4e8b-8217-fa1af04e9767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alkaff\\AppData\\Local\\Temp\\ipykernel_19352\\4254497666.py:4: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  get_weather(\"Singapore\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Weather in Singapore: 30.3¬∞C, Partly cloudy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the Weather API\n",
    "# --------------------\n",
    "\n",
    "get_weather(\"Singapore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dda3a7-ad35-4be7-bc14-8b3fdd33b362",
   "metadata": {},
   "source": [
    "## üìå Part 2: Adding OpenAI Function Definition \n",
    "\n",
    "- **Note:** I have decided to use an open source model Qwen 1.5 1.8b chat instead of OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95c732-6b89-4d98-b3e1-67ec7336055f",
   "metadata": {},
   "source": [
    "### Step 1: Load the Qwen 1.5 1.8b Chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0268d4f7-cf43-4940-9f91-4ef445e75443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load LLM - Qwen 1.5 1.8B Chat\n",
    "# -----------------------------\n",
    "\n",
    "# Load Model\n",
    "model_id = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "tok  = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",          # \"cuda\" if GPU, else \"cpu\"\n",
    "            torch_dtype=\"auto\"\n",
    "        )\n",
    "\n",
    "# Create Qwen Pipeline\n",
    "qwen_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tok,\n",
    "    #max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    #device=0  # device=0 for GPU\n",
    ")\n",
    "\n",
    "# Create the LLM\n",
    "llm = HuggingFacePipeline(pipeline=qwen_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a36285-109b-4368-960c-c476c9523393",
   "metadata": {},
   "source": [
    "### Step 2: Create the Agent by using `create_react_agent` instead of `create_tool_calling_agent`\n",
    "\n",
    "- **Qwen doesn't natively support function call**\n",
    "- `create_react_agent` is for models that doesn't support native function calls whereas `create_tool_calling_agent` is for models that support native function calls like OpenAI or Antrophic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59f4d69-bd57-4e23-9096-45c904eaae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Prompt Template\n",
    "# ----------------------\n",
    "\n",
    "prompt_text = \"\"\"\n",
    "You are a helpful AI assistant.\n",
    "You have access to these tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "The tool names are: {tool_names}\n",
    "\n",
    "IMPORTANT INSTRUCTIONS:\n",
    "- You must ALWAYS produce the following steps in order:\n",
    "  1. Thought: describe your reasoning.\n",
    "  2. Action: the name of the action to take.\n",
    "  3. Action Input: the input for the action.\n",
    "- NEVER skip the Action or Action Input.\n",
    "- NEVER write a Final Answer before you see an Observation.\n",
    "- After you see the Observation, write a Thought and then a Final Answer in natural language describing the weather.\n",
    "- If the question does not require any tool, say \"I don't know.\"\n",
    "\n",
    "Example:\n",
    "\n",
    "Question: What is the weather in Paris?\n",
    "Thought: I need to get the weather for Paris.\n",
    "Action: get_weather\n",
    "Action Input: \"Paris\"\n",
    "\n",
    "Use this format exactly:\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6fca64-5859-4093-b1c8-e81717a201cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create React Agent\n",
    "# ------------------\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=[get_weather],\n",
    "    prompt=prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "503e23bb-d323-435f-adce-6d17ccb33ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Executor for React Agent\n",
    "# -------------------------------\n",
    "\n",
    "executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=[get_weather],\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be92354e-1288-4428-8416-8bf3fa0fd75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mFinal Answer: The weather in Tokyo is currently [current weather description]. Keep in mind that forecasts can\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the weather in Tokyo?\",\n",
       " 'output': 'The weather in Tokyo is currently [current weather description]. Keep in mind that forecasts can'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Sample Output\n",
    "# ------------------\n",
    "\n",
    "executor.invoke({\"input\": \"What's the weather in Tokyo?\"})\n",
    "#executor.run(\"What's the weather in Tokyo?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40864e5-a7c1-4f6d-8621-5969bee37986",
   "metadata": {},
   "source": [
    "### üìù Observations\n",
    "\n",
    "In this step, I initially attempted to use the **ReAct agent** approach with LangChain. However, I observed that the ReAct agent consistently failed to produce reliable outputs when used with an open-source model (Qwen 1.5 Chat). \n",
    "\n",
    "Specifically:\n",
    "\n",
    "- The model frequently returned outputs that mixed **Thought**, **Action**, and **Final Answer** in inconsistent ways.\n",
    "- It sometimes generated a **Final Answer** *and* an **Action** simultaneously, which caused parsing errors.\n",
    "- It also sometimes hallucinated where the final output still contained placeholders instead of actual values. \n",
    "- The ReAct agent expects the LLM to strictly follow a special format (Thought ‚Üí Action ‚Üí Action Input ‚Üí Observation), but open models often deviate from this format.\n",
    "- ReAct loops can get stuck in retries or error chains if the output does not match expectations.\n",
    "\n",
    "Because of these reliability issues, I decided to switch to a **JSON-based approach** instead:\n",
    "\n",
    "‚úÖ In this alternative workflow:\n",
    "- The model is prompted to **output only JSON** indicating which function to call.\n",
    "- The function is called manually in Python.\n",
    "- The result is summarized by prompting the model again.\n",
    "\n",
    "This 2-step approach proved more robust and predictable with an open-source LLM and allowed for clear separation of concerns (decision, execution, summarization).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ed3d8-2f97-4a10-9fe3-da8b5603be77",
   "metadata": {},
   "source": [
    "### Step 2 (2nd Attempt): Create the Agent by using the JSON-based approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "857fd2ad-ba6c-49de-8195-9026cdf3473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON Agent\n",
    "# ----------\n",
    "\n",
    "prompt_json_text = \"\"\"\n",
    "You are a function-calling assistant.\n",
    "\n",
    "Given a user question, respond ONLY in JSON, specifying which function to call and what arguments.\n",
    "\n",
    "The available functions are:\n",
    "\n",
    "- get_weather(location: str): Get the current weather for a location.\n",
    "\n",
    "IMPORTANT: Start your answer with \"Answer:\" and nothing else.\n",
    "\n",
    "Return your answer in this format (no extra text):\n",
    "\n",
    "{{\n",
    "  \"function\": \"<function name>\",\n",
    "  \"arguments\": {{\"location\": \"<value>\"}}\n",
    "}}\n",
    "\n",
    "If you don't know which function to call, respond with:\n",
    "\n",
    "{{\"function\": \"none\", \"arguments\": {{}}}}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_json = PromptTemplate.from_template(prompt_json_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9c35eec-5c19-4cd8-b900-7f1080d31786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Agent\n",
    "# -------------\n",
    "\n",
    "prompt_summary_text = \"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Using the corresponding data:\n",
    "\n",
    "{data}\n",
    "\n",
    "Write the shortest possible, clear natural language answer for the user.\n",
    "\n",
    "IMPORTANT: Respond only with a single sentence prefixed by \"Answer:\" and no other additional text.\n",
    "\"\"\"\n",
    "\n",
    "prompt_summary = PromptTemplate.from_template(prompt_summary_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3ddcea9-b59a-4399-ab51-743cf7229f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Executor\n",
    "# -----------------\n",
    "\n",
    "def process_action(raw_json):\n",
    "    try:\n",
    "        parsed = json.loads(raw_json)\n",
    "        func = parsed[\"function\"]\n",
    "        args = parsed[\"arguments\"]\n",
    "\n",
    "        if func == \"get_weather\":\n",
    "            return get_weather(args[\"location\"])\n",
    "        else:\n",
    "            return \"No relevant function to call.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error parsing JSON: {e}\\nRaw: {raw_json}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ba61523-8481-4a95-8c73-0ee94b89fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt = What's the weather in Tokyo?\n",
      "res_json = {\"function\": \"get_weather\", \"arguments\": {\"location\": \"Tokyo\"}}\n",
      "res_final = The weather in Tokyo is currently 26.4¬∞C with partly cloudy skies.\n"
     ]
    }
   ],
   "source": [
    "# Testing Sample\n",
    "# --------------\n",
    "\n",
    "prompt = \"What's the weather in Tokyo?\"\n",
    "print(f\"prompt = {prompt}\")\n",
    "\n",
    "prompt_obj = prompt_json.format(question=prompt)\n",
    "res_json = llm(prompt_obj)\n",
    "res_json = res_json.split(\"Answer:\")[-1].strip()\n",
    "print(f\"res_json = {res_json}\")\n",
    "\n",
    "prompt2 = process_action(res_json)\n",
    "\n",
    "prompt_obj2 = prompt_summary.format(\n",
    "    data=prompt2,\n",
    "    question=prompt\n",
    ")\n",
    "res_final = llm(prompt_obj2)\n",
    "res_final = res_final.split(\"Answer:\")[-1].strip()\n",
    "print(f\"res_final = {res_final}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264943c1-4bfc-48ea-9467-ac3769cc9172",
   "metadata": {},
   "source": [
    "## üìå Part 3: Implementing the Weather Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee762971-48f0-4dcf-a24d-9561735f0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather Agent\n",
    "# -------------\n",
    "\n",
    "def weather_agent(user_query):\n",
    "    try:\n",
    "        # Decide which function to call\n",
    "        prompt_decision = prompt_json.format(question=user_query)\n",
    "        res_json = llm(prompt_decision)\n",
    "        res_json = res_json.split(\"Answer:\")[-1].strip()\n",
    "        \n",
    "        # Process the JSON output to call the function\n",
    "        action_result = process_action(res_json)\n",
    "\n",
    "        # If no relevant function or error, return directly\n",
    "        if \"No relevant function\" in action_result or \"Error\" in action_result:\n",
    "            return action_result\n",
    "\n",
    "        # Summarize the result\n",
    "        prompt_summary_filled = prompt_summary.format(\n",
    "            data=action_result,\n",
    "            question=user_query\n",
    "        )\n",
    "        res_final = llm(prompt_summary_filled)\n",
    "        res_final = res_final.split(\"Answer:\")[-1].strip()\n",
    "        \n",
    "        return res_final\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error in weather_agent: {e}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa599501-de29-4bcd-b6d8-8e0c5aefe305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üó£Ô∏è  USER: What's the weather like in Singapore?\n",
      "==================================================\n",
      "ü§ñ AGENT: The weather in Singapore is partly cloudy with an average temperature of 29.1¬∞C.\n",
      "==================================================\n",
      "==================================================\n",
      "üó£Ô∏è  USER: Is it raining in London right now?\n",
      "==================================================\n",
      "ü§ñ AGENT: Yes, it is raining in London right now according to the current weather data provided.\n",
      "==================================================\n",
      "==================================================\n",
      "üó£Ô∏è  USER: Tell me the temperature in New York\n",
      "==================================================\n",
      "ü§ñ AGENT: The current temperature in New York is 27.2¬∞C, with partly cloudy skies.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test Queries\n",
    "# ------------\n",
    "\n",
    "test_queries = [\n",
    "    \"What's the weather like in Singapore?\",\n",
    "    \"Is it raining in London right now?\",\n",
    "    \"Tell me the temperature in New York\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üó£Ô∏è  USER: {query}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"ü§ñ AGENT: {weather_agent(query)}\")\n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ea834-e9a8-4aaf-a995-560ed855503d",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "For this assignment, I decided to **challenge myself** by using an **open-source model (Qwen 1.5 Chat)** instead of the recommended OpenAI GPT-4. Key observations and adaptations:\n",
    "\n",
    "- ‚úÖ **Model Choice:** Selected Qwen 1.5 Chat to explore using open-source alternatives.\n",
    "- ‚úÖ **Initial Approach:** Tried the standard React Agent workflow with LangChain.\n",
    "- ‚ö†Ô∏è **Issue:** React Agent produced inconsistent outputs, parsing errors, and repeated loops when used with Qwen.\n",
    "- ‚úÖ **Solution:** Switched to a **JSON-based function calling workflow**:\n",
    "  - The model responds only in a strict JSON format specifying the function and arguments.\n",
    "  - A separate step processes the JSON and calls the appropriate function.\n",
    "  - Another step uses the model to generate a clean natural language answer.\n",
    "- ‚úÖ **Result:** This approach provided:\n",
    "  - More predictable and reliable outputs.\n",
    "  - Clear separation of function selection and response formatting.\n",
    "  - Better control over error handling.\n",
    "- ‚úÖ **Takeaway:** With careful prompt engineering and workflow design, open-source models can be integrated successfully into agent-based systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ede94-d0b8-4988-826d-9c46af457992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
